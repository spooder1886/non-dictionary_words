{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Подготовка словарей**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156016\n"
     ]
    }
   ],
   "source": [
    "#Лопатин, если вдруг будет нам нужен\n",
    "with open('lopatin.txt', 'r', encoding='utf-8') as file:\n",
    "    dic = file.read()\n",
    "\n",
    "#начало костыля\n",
    "pattern = r'а́'\n",
    "subp = 'а'\n",
    "dic = re.sub(pattern, subp, dic)\n",
    "\n",
    "pattern = r'А́'\n",
    "subp = 'А'\n",
    "dic = re.sub(pattern, subp, dic)\n",
    "\n",
    "pattern = r'е́'\n",
    "subp = 'е'\n",
    "dic = re.sub(pattern, subp, dic)\n",
    "\n",
    "pattern = r'Е́'\n",
    "subp = 'Е'\n",
    "dic = re.sub(pattern, subp, dic)\n",
    "\n",
    "pattern = r'и́'\n",
    "subp = 'и'\n",
    "dic = re.sub(pattern, subp, dic)\n",
    "\n",
    "pattern = r'И́'\n",
    "subp = 'И'\n",
    "dic = re.sub(pattern, subp, dic)\n",
    "\n",
    "pattern = r'о́'\n",
    "subp = 'и'\n",
    "dic = re.sub(pattern, subp, dic)\n",
    "\n",
    "pattern = r'О́'\n",
    "subp = 'О'\n",
    "dic = re.sub(pattern, subp, dic)\n",
    "\n",
    "pattern = r'у́'\n",
    "subp = 'у'\n",
    "dic = re.sub(pattern, subp, dic)\n",
    "\n",
    "pattern = r'У́'\n",
    "subp = 'У'\n",
    "dic = re.sub(pattern, subp, dic)\n",
    "\n",
    "pattern = r'ы́'\n",
    "subp = 'ы'\n",
    "dic = re.sub(pattern, subp, dic)\n",
    "\n",
    "pattern = r'э́'\n",
    "subp = 'э'\n",
    "dic = re.sub(pattern, subp, dic)\n",
    "\n",
    "pattern = r'Э́'\n",
    "subp = 'Э'\n",
    "dic = re.sub(pattern, subp, dic)\n",
    "\n",
    "pattern = r'ю́'\n",
    "subp = 'ю'\n",
    "dic = re.sub(pattern, subp, dic)\n",
    "\n",
    "pattern = r'Ю́'\n",
    "subp = 'Ю'\n",
    "dic = re.sub(pattern, subp, dic)\n",
    "\n",
    "pattern = r'я́'\n",
    "subp = 'я'\n",
    "dic = re.sub(pattern, subp, dic)\n",
    "\n",
    "pattern = r'Я́'\n",
    "subp = 'Я'\n",
    "dic = re.sub(pattern, subp, dic)\n",
    "#конец костыля\n",
    "\n",
    "pattern = r'\\n([А-Я])\\n'\n",
    "subp = '\\n'\n",
    "dic = re.sub(pattern, subp, dic)\n",
    "\n",
    "pattern = r'\\n([а-яА-ЯёЁ][а-яА-ЯёЁ-]*)'\n",
    "lst = re.findall(pattern, dic)\n",
    "\n",
    "for i in range(len(lst)):\n",
    "    lst[i] = lst[i].strip()\n",
    "\n",
    "set_lopatin = set(lst)\n",
    "print(len(set_lopatin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38380\n"
     ]
    }
   ],
   "source": [
    "#Ожегов\n",
    "with open('ojegov.txt', 'r', encoding='ansi') as file:\n",
    "    dic = file.read()\n",
    "\n",
    "pattern = r'\\n\\n\\n\\w\\n'\n",
    "dic = re.sub(pattern, '', dic)\n",
    "\n",
    "dic = dic.lower()\n",
    "\n",
    "pattern = r'\\n([а-яА-ЯёЁ][а-яА-ЯёЁ-]*)'\n",
    "lst = re.findall(pattern, dic)\n",
    "\n",
    "set_oj = set(lst)\n",
    "print(len(set_oj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676\n"
     ]
    }
   ],
   "source": [
    "#Идиоматика\n",
    "with open('idiomatika.txt', 'r', encoding='utf-8') as file:\n",
    "    dic = file.read()\n",
    "\n",
    "pattern = r'\\n([а-яА-ЯёЁ][а-яА-ЯёЁ-]*)'\n",
    "lst = re.findall(pattern, dic)\n",
    "\n",
    "for i in range(len(lst)):\n",
    "    lst[i] = lst[i].strip()\n",
    "\n",
    "set_idiomatika = set(lst)\n",
    "print(len(set_idiomatika))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120579\n"
     ]
    }
   ],
   "source": [
    "#Ефремова\n",
    "with open('efremova.txt', 'r', encoding='utf-8') as file:\n",
    "    dic = file.read()\n",
    "\n",
    "pattern = r'\\n([а-яА-ЯёЁ][а-яА-ЯёЁ-]*)'\n",
    "lst = re.findall(pattern, dic)\n",
    "\n",
    "for i in range(len(lst)):\n",
    "    lst[i] = lst[i].strip()\n",
    "\n",
    "set_efremova = set(lst)\n",
    "print(len(set_efremova))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78036\n"
     ]
    }
   ],
   "source": [
    "#БТС\n",
    "with open('bts.txt', 'r', encoding='cp1251') as file:\n",
    "    dic = file.read()\n",
    "\n",
    "pattern = r'\\n([а-яА-ЯёЁ][а-яА-ЯёЁ-]*)'\n",
    "lst = re.findall(pattern, dic)\n",
    "\n",
    "for i in range(len(lst)):\n",
    "    lst[i] = lst[i].strip()\n",
    "\n",
    "set_bts = set(lst)\n",
    "print(len(set_bts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ЧЕРЕЗ РЕГУЛЯРКИ. Города, имена, аббревиатуры\n",
    "import pickle\n",
    "abbrevy = pickle.load(open('abbrev', 'rb'))\n",
    "set_abbrev = set(abbrevy)\n",
    "\n",
    "with open('cities.txt', 'r', encoding='utf-8') as file:\n",
    "    dic = file.read()\n",
    "\n",
    "pattern = r'\\n([а-яА-ЯёЁ][а-яА-ЯёЁ-]*)'\n",
    "lst = re.findall(pattern, dic)\n",
    "\n",
    "for i in range(len(lst)):\n",
    "    lst[i] = lst[i].strip()\n",
    "\n",
    "set_cities = set(lst)\n",
    "\n",
    "with open('names.txt', 'r', encoding='utf-8') as file:\n",
    "    dic = file.read()\n",
    "\n",
    "pattern = r'\\n([а-яА-ЯёЁ][а-яА-ЯёЁ-]*)'\n",
    "lst = re.findall(pattern, dic)\n",
    "\n",
    "for i in range(len(lst)):\n",
    "    lst[i] = lst[i].strip()\n",
    "\n",
    "set_names = set(lst)\n",
    "\n",
    "with open('russian_surnames.txt', 'r', encoding='utf-8') as file:\n",
    "    dic = file.read()\n",
    "\n",
    "pattern = r'\\n([а-яА-ЯёЁ][а-яА-ЯёЁ-]*)'\n",
    "lst = re.findall(pattern, dic)\n",
    "\n",
    "for i in range(len(lst)):\n",
    "    lst[i] = lst[i].strip()\n",
    "\n",
    "set_surnames = set(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ОБНОВЛЕННЫЙ Города, имена, аббревиатуры\n",
    "import pickle\n",
    "abbrevy = pickle.load(open('abbrev', 'rb'))\n",
    "set_abbrev = set(abbrevy)\n",
    "\n",
    "with open('cities.txt', 'r', encoding='utf-8') as file:\n",
    "    dic = [x.strip().lower() for x in file.readlines()]\n",
    "set_cities = set(dic)\n",
    "\n",
    "\n",
    "with open('russian_surnames.txt', 'r', encoding='utf-8') as file:\n",
    "    dic = [x.strip().lower() for x in file.readlines()]\n",
    "set_surnames = set(dic)\n",
    "\n",
    "with open('countries.txt', 'r', encoding='utf-8') as file:\n",
    "    dic = [x.strip().lower() for x in file.readlines()]\n",
    "set_countries = set(dic)\n",
    "\n",
    "with open('names.txt', 'r', encoding='utf-8') as file:\n",
    "    dic = [x.strip().lower() for x in file.readlines()]\n",
    "set_names = set(dic)\n",
    "\n",
    "with open('midnames.txt', 'r', encoding='utf-8') as file:\n",
    "    dic = [x.strip().lower() for x in file.readlines()]\n",
    "set_midnames = set(dic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251337\n",
      "51529\n"
     ]
    }
   ],
   "source": [
    "print(len(set_surnames))\n",
    "#print(set_surnames)\n",
    "print(len(set_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435968\n"
     ]
    }
   ],
   "source": [
    "final_dic = set_oj.union(set_idiomatika)\n",
    "final_dic |= set_efremova\n",
    "final_dic |= set_bts\n",
    "final_dic |= set_cities\n",
    "final_dic |= set_names\n",
    "final_dic |= set_abbrev\n",
    "final_dic |= set_surnames\n",
    "final_dic |= set_countries\n",
    "final_dic |= set_midnames\n",
    "\n",
    "final_dic = {word.replace('ё', 'е') for word in final_dic}\n",
    "\n",
    "with open('final_dic.txt', 'w', encoding='utf-8') as file:\n",
    "    for item in final_dic:\n",
    "        file.write(f'{item}\\n')\n",
    "\n",
    "print(len(final_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93067\n"
     ]
    }
   ],
   "source": [
    "#Зализняк, если вдруг будет нам нужен\n",
    "with open('Zaliz.txt', 'r', encoding='utf-8') as file:\n",
    "    dic = file.read()\n",
    "\n",
    "pattern = r'\\n([а-яА-ЯёЁ][а-яА-ЯёЁ-]*)'\n",
    "lst = re.findall(pattern, dic)\n",
    "\n",
    "for i in range(len(lst)):\n",
    "    lst[i] = lst[i].strip()\n",
    "\n",
    "zal_set = set(lst)\n",
    "print(len(zal_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3351919\n"
     ]
    }
   ],
   "source": [
    "#Компрено\n",
    "with open(r'C:\\Users\\spood\\Desktop\\Диплом\\compreno\\MorphoDic_OUT.txt', 'r', encoding='utf-8') as file:\n",
    "    dic = file.read()\n",
    "\n",
    "\n",
    "pattern = r'\\n([а-яА-ЯёЁ][а-яА-ЯёЁ-]*)'\n",
    "lst = re.findall(pattern, dic)\n",
    "\n",
    "for i in range(len(lst)):\n",
    "    lst[i] = lst[i].strip()\n",
    "\n",
    "set_comprenodic = set(lst)\n",
    "print(len(set_comprenodic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3320892\n"
     ]
    }
   ],
   "source": [
    "word_dic = set_comprenodic.union(zal_set)\n",
    "word_dic = {word.lower().replace('ё', 'е') for word in word_dic}\n",
    "\n",
    "word_list = sorted(word_dic)\n",
    "\n",
    "with open('word_dic.txt', 'w', encoding='utf-8') as file:\n",
    "    for item in word_list:\n",
    "        file.write(f'{item}\\n')\n",
    "\n",
    "print(len(word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Скрипт по поиску слов в ЖЗ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20.05 версия, с выводом словоформ для леммы\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "class Magazine:\n",
    "    def __init__(self, path, final_dic, zal_set, set_comprenodic, word_dic):\n",
    "        self.path = path  # путь к папке с ЖЗ\n",
    "        self.final_dic = final_dic  # все словари (кроме Зализняка и Компрено)\n",
    "        self.zal_set = zal_set  # словарь Зализняка\n",
    "        self.set_comprenodic = set_comprenodic  # словарь Компрено\n",
    "        self.word_dic = word_dic  # комбинация Зализняка и Компрено\n",
    "        self.results = defaultdict(lambda: {'total': 0, 'authors': defaultdict(int), 'forms': set()})  # здесь результаты, включая авторов и формы\n",
    "\n",
    "    def gather_and_check_lemmas(self):\n",
    "        lemmas = set()\n",
    "        for root, dirs, files in os.walk(self.path):\n",
    "            for file in files:\n",
    "                if file.endswith('.txt'):\n",
    "                    with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                        author_id = None\n",
    "                        for line in f:\n",
    "                            if 'authorid=' in line:\n",
    "                                author_id = line.split()[1]  # айди автора на втором месте в строке\n",
    "                            if line[0].isdigit():\n",
    "                                parts = line.strip().split()\n",
    "                                lemma = parts[2].lower().replace('ё', 'е')\n",
    "                                word = parts[1].lower().replace('ё', 'е')\n",
    "                                # Проверяем, есть ли лемма в словарях Компрено и Зализняка\n",
    "                                if lemma not in self.final_dic and word not in self.word_dic:\n",
    "                                    self.results[lemma]['total'] += 1\n",
    "                                    self.results[lemma]['forms'].add(word)\n",
    "                                    if author_id:\n",
    "                                        self.results[lemma]['authors'][author_id] += 1\n",
    "\n",
    "        # Удаляем слова, которые встретились у двух и менее авторов\n",
    "        words_to_remove = [word for word, data in self.results.items() if len(data['authors']) <= 2]\n",
    "        for word in words_to_remove:\n",
    "            del self.results[word]\n",
    "\n",
    "        return lemmas\n",
    "\n",
    "    def save_results(self, filename='diploma_results.txt'):\n",
    "        sorted_results = sorted(self.results.items(), key=lambda x: x[1]['total'], reverse=True)\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            for word, counts in sorted_results:\n",
    "                forms = \", \".join(counts['forms'])\n",
    "                f.write(f\"{word} - {counts['total']} - {len(counts['authors'])} - {forms}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "афыва"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20.05 версия, с выводом словоформ для леммы\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "class Magazine:\n",
    "    def __init__(self, path, final_dic, zal_set, set_comprenodic, word_dic):\n",
    "        self.path = path  # путь к папке с ЖЗ\n",
    "        self.final_dic = final_dic  # все словари (кроме Зализняка и Компрено)\n",
    "        self.zal_set = zal_set  # словарь Зализняка\n",
    "        self.set_comprenodic = set_comprenodic  # словарь Компрено\n",
    "        self.word_dic = word_dic  # комбинация Зализняка и Компрено\n",
    "        self.results = defaultdict(lambda: {'total': 0, 'authors': defaultdict(int), 'forms': set()})  # здесь результаты, включая авторов и формы\n",
    "\n",
    "    def gather_and_check_lemmas(self):\n",
    "        for root, dirs, files in os.walk(self.path):\n",
    "            for file in files:\n",
    "                if file.endswith('.txt'):\n",
    "                    with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                        author_id = None\n",
    "                        for line in f:\n",
    "                            if 'authorid=' in line:\n",
    "                                author_id = line.split()[1]  # айди автора на втором месте в строке\n",
    "                            if line[0].isdigit():\n",
    "                                parts = line.strip().split()\n",
    "                                lemma = parts[2].lower().replace('ё', 'е')\n",
    "                                word = parts[1].lower().replace('ё', 'е')\n",
    "                                # Проверяем, есть ли лемма в словарях Компрено и Зализняка\n",
    "                                if lemma not in self.final_dic and word not in self.word_dic:\n",
    "                                    self.results[lemma]['total'] += 1\n",
    "                                    self.results[lemma]['forms'].add(word)\n",
    "                                    if author_id:\n",
    "                                        self.results[lemma]['authors'][author_id] += 1\n",
    "\n",
    "        # Удаляем слова, которые встретились у двух и менее авторов\n",
    "        words_to_remove = [word for word, data in self.results.items() if len(data['authors']) <= 2]\n",
    "        for word in words_to_remove:\n",
    "            del self.results[word]\n",
    "\n",
    "        return lemmas\n",
    "\n",
    "    def save_results(self, filename='diploma_results.txt'):\n",
    "        sorted_results = sorted(self.results.items(), key=lambda x: x[1]['total'], reverse=True)\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            for word, counts in sorted_results:\n",
    "                forms = \", \".join(counts['forms'])\n",
    "                f.write(f\"{word} - {counts['total']} - {len(counts['authors'])} - {forms}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dic = final_dic\n",
    "zal_set = zal_set\n",
    "set_comprenodic = set_comprenodic\n",
    "word_dic = word_dic\n",
    "\n",
    "magazine = Magazine(path='F:\\\\jz_rightlemmas_all', word_dic=word_dic, final_dic=final_dic, zal_set=zal_set, set_comprenodic=set_comprenodic)\n",
    "lemmas = magazine.gather_and_check_lemmas()\n",
    "magazine.save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def filter_cyrillic_lines(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    cyrillic_lines = []\n",
    "    for line in lines:\n",
    "        if re.match(r'^[а-яА-Я]', line) and '.' not in line:  # Проверяем\n",
    "            cyrillic_lines.append(line)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(cyrillic_lines)\n",
    "\n",
    "\n",
    "input_file = 'diploma_results.txt'\n",
    "output_file = 'diploma_results.txt'\n",
    "filter_cyrillic_lines(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_words(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    filtered_lines = [line for line in lines if len(line.split()[0]) > 2]\n",
    "\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(filtered_lines)\n",
    "\n",
    "\n",
    "remove_short_words(\"diploma_results.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже архив версий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#первая странная версия\n",
    "def parse_conllu_file(file_path):\n",
    "    # Парсит файл формата CoNLL-U и возвращает словарь лемм с соответствующими словоформами\n",
    "    lemma_dict = {}\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # Обходим строки в файле\n",
    "        for line in file:\n",
    "            # Разбиваем строку по табуляции\n",
    "            parts = line.strip().split('\\t')\n",
    "\n",
    "            # Проверяем наличие необходимых столбцов\n",
    "            if len(parts) >= 3:\n",
    "                # Извлекаем лемму и форму слова\n",
    "                lemma = parts[2]\n",
    "                form = parts[1]\n",
    "\n",
    "                # Добавляем в словарь\n",
    "                if lemma not in lemma_dict:\n",
    "                    lemma_dict[lemma] = set()\n",
    "\n",
    "                lemma_dict[lemma].add(form)\n",
    "\n",
    "    return lemma_dict\n",
    "\n",
    "def save_result_to_file(result_dict, output_file_path):\n",
    "\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        for lemma, forms in result_dict.items():\n",
    "            output_file.write(f'{lemma}: {forms}\\n')\n",
    "\n",
    "file_path = 'datatest.txt'\n",
    "output_file_path = 'datatest_results.txt'\n",
    "\n",
    "result_dict = parse_conllu_file(file_path)\n",
    "save_result_to_file(result_dict, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#07.05 версия\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "class Magazine:\n",
    "    def __init__(self, path, final_dic, zal_set, set_comprenodic):\n",
    "        self.path = path # путь к папке с ЖЗ\n",
    "        self.lemmadicts = self.gatherlemmas() # тут будем собирать  леммы из ЖЗ\n",
    "        self.final_dic = final_dic # все словари (кроме Зализняка и Компрено)\n",
    "        self.zal_set = zal_set # словарь Зализняка\n",
    "        self.set_comprenodic = set_comprenodic # словарь Компрено\n",
    "        self.results = defaultdict(dict) # здесь результаты\n",
    "\n",
    "\n",
    "    def gatherlemmas(self):\n",
    "        lemmas = set()\n",
    "        # тут считываем данные из файлов в формате CoNLL-U и извлекаем слова\n",
    "        for root, dirs, files in os.walk(self.path):\n",
    "            for file in files:\n",
    "                if file.endswith('.txt'): # учитываем, что файлы .txt\n",
    "                    with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                        for line in f:\n",
    "                            # тут можно было бы распарсивать автора: если строчка начинается на '<text', то в этой строчке содержится его айдишник. \n",
    "                            # я бы сохраняла этот айдишник в переменную; в словаре лемм можно завести допключ 'authors', \n",
    "                            # где бы хранился словарь с ключами - айди авторов и значениями - количеством встретившегося у них\n",
    "                            # типа\n",
    "                            # authorid = ... (re.search())\n",
    "                            parts = line.strip().split()\n",
    "                            if len(parts) >= 2: # а можно было определить словари до lemmadicts и тогда сразу же тут проверять леммы и не добавлять их... опять мартышкин труд\n",
    "                                # я бы всю работу по сбору итогового результата сделала здесь: \n",
    "                                # во-первых, надо еще parts[1] (словоформу) проверять по компрене и зализняку\n",
    "                                # во-вторых, можно тут же сразу отсеивать и леммы\n",
    "                                word = parts[2] # забираем слово с третьей позиции\n",
    "                                lemmas.add(word)\n",
    "                                # а потом if 'authorid' in self.results[word]:\n",
    "                                # self.results[word]['authorid'][authorid] = self.results[word]['authorid'].setdefault(authorid) + 1\n",
    "        return lemmas\n",
    "\n",
    "\n",
    "    def check(self):\n",
    "        # Сначала соберем леммы\n",
    "        lemmas = self.lemmadicts.copy()\n",
    "\n",
    "        # Затем сравним леммы с final_dic, Компрено и Зализняком и удалим из них совпадающие слова\n",
    "        for word in self.final_dic:\n",
    "            if word in lemmas:\n",
    "                lemmas.remove(word)\n",
    "        for word in self.set_comprenodic:\n",
    "            if word in lemmas:\n",
    "                lemmas.remove(word)\n",
    "        for word in self.zal_set:\n",
    "            if word in lemmas:\n",
    "                lemmas.remove(word)\n",
    "\n",
    "        # Теперь у нас в lemmas содержатся только те леммы, которых еще нет в списках final_dic, Компрено и Зализняке\n",
    "\n",
    "        # Дальше делаем то же, что и в gatherlemmas, используя уже очищенные леммы\n",
    "        for root, dirs, files in os.walk(self.path): # вы дважды просматриваете весь жз. Дважды. А ведь можно было это все объединить в один цикл...\n",
    "            for file in files:\n",
    "                if file.endswith('.txt'): \n",
    "                    with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                        for line in f:\n",
    "                            parts = line.strip().split()\n",
    "                            if len(parts) >= 2:\n",
    "                                word = parts[2]\n",
    "                                if word in lemmas:\n",
    "                                    if 'total' not in self.results[word]:\n",
    "                                        self.results[word]['total'] = 1\n",
    "                                    else:\n",
    "                                        self.results[word]['total'] += 1\n",
    "\n",
    "\n",
    "    def save_results(self, filename='datatest_compared_results.txt'):\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            for word, counts in self.results.items():\n",
    "                f.write(f\"{word} - {counts['total']}\\n\")\n",
    "\n",
    "\n",
    "        sorted_results = sorted(self.results.items(), key=lambda x: x[1]['total'], reverse=True)\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            for word, counts in sorted_results:\n",
    "                f.write(f\"{word} - {counts['total']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#для 07.05\n",
    "import re\n",
    "\n",
    "magazine = Magazine(path='F:\\\\jz_rightlemmas_all\\\\10', final_dic=final_dic, zal_set=zal_set, set_comprenodic=set_comprenodic)\n",
    "lemmas = magazine.gatherlemmas()\n",
    "magazine.check()\n",
    "magazine.save_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#15.05 версия\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "class Magazine:\n",
    "    def __init__(self, path, final_dic, zal_set, set_comprenodic, word_dic):\n",
    "        self.path = path  # путь к папке с ЖЗ\n",
    "        self.final_dic = final_dic  # все словари (кроме Зализняка и Компрено)\n",
    "        self.zal_set = zal_set  # словарь Зализняка\n",
    "        self.set_comprenodic = set_comprenodic  # словарь Компрено\n",
    "        self.word_dic = word_dic # комбинация Зализняка и Компрено\n",
    "        self.results = defaultdict(lambda: {'total': 0, 'authors': defaultdict(int)})  # здесь результаты, включая авторов\n",
    "\n",
    "    def gather_and_check_lemmas(self):\n",
    "        lemmas = set()\n",
    "        for root, dirs, files in os.walk(self.path):\n",
    "            for file in files:\n",
    "                if file.endswith('.txt'):\n",
    "                    with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                        author_id = None\n",
    "                        for line in f:\n",
    "                            if line.startswith('<text'):\n",
    "                                author_id = line.split()[1]  # айди автора на втором месте в строке\n",
    "                            parts = line.strip().split()\n",
    "                            if len(parts) >= 3:  # предполагаем, что третья часть - это лемма\n",
    "                                lemma = parts[2].lower().replace('ё', 'е')\n",
    "                                word = parts[1].lower().replace('ё', 'е')\n",
    "                                # Проверяем, есть ли лемма в словарях Компрено и Зализняка\n",
    "                                if lemma not in self.final_dic and word not in word_dic:\n",
    "                                    self.results[lemma]['total'] += 1\n",
    "                                    if author_id:\n",
    "                                        self.results[lemma]['authors'][author_id] += 1\n",
    "\n",
    "        # Удаляем слова, которые встретились у двух и менее авторов\n",
    "        words_to_remove = [word for word, data in self.results.items() if len(data['authors']) <= 2]\n",
    "        for word in words_to_remove:\n",
    "            del self.results[word]\n",
    "\n",
    "        return lemmas\n",
    "\n",
    "    def save_results(self, filename='diploma_results.txt'):\n",
    "        sorted_results = sorted(self.results.items(), key=lambda x: x[1]['total'], reverse=True)\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            for word, counts in sorted_results:\n",
    "                f.write(f\"{word} - {counts['total']} - {len(self.results[word]['authors'])}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
